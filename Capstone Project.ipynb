{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import boto3\n",
    "import psycopg2\n",
    "import configparser\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The aim of this project is the development of a Datawarehouse for US immigration data. The DWH is designed for correlation of data of incoming immigrants, their origin countries and destination cities demographics. This will allow for Business Intelligence reports.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The datasets used in this project are provided by Udacity for the capstone project. The datasets are:\n",
    "\n",
    "* I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace.\n",
    "* U.S. City Demographic Data: This data comes from OpenSoft.\n",
    "* Airport Code Table: This is a simple table of airport codes and corresponding cities.\n",
    "\n",
    "Udacity also provides a world temperature dataset which was not used in this project, because the I94 data is from the year 2016, and the temperature dataset contains data up to 2013. Therefore, no conclusions can be obtained from associating this data to the immigration data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      "ident           55075 non-null object\n",
      "type            55075 non-null object\n",
      "name            55075 non-null object\n",
      "elevation_ft    48069 non-null float64\n",
      "continent       27356 non-null object\n",
      "iso_country     54828 non-null object\n",
      "iso_region      55075 non-null object\n",
      "municipality    49399 non-null object\n",
      "gps_code        41030 non-null object\n",
      "iata_code       9189 non-null object\n",
      "local_code      28686 non-null object\n",
      "coordinates     55075 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Airport Code Table dataset view\n",
    "df_airport = pd.read_csv(\"airport-codes_csv.csv\")\n",
    "display(df_airport.info())\n",
    "display(df_airport.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      "City                      2891 non-null object\n",
      "State                     2891 non-null object\n",
      "Median Age                2891 non-null float64\n",
      "Male Population           2888 non-null float64\n",
      "Female Population         2888 non-null float64\n",
      "Total Population          2891 non-null int64\n",
      "Number of Veterans        2878 non-null float64\n",
      "Foreign-born              2878 non-null float64\n",
      "Average Household Size    2875 non-null float64\n",
      "State Code                2891 non-null object\n",
      "Race                      2891 non-null object\n",
      "Count                     2891 non-null int64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# US City Demographics dataset view\n",
    "df_city_demographics = pd.read_csv(\"us-cities-demographics.csv\", delimiter=\";\")\n",
    "display(df_city_demographics.info())\n",
    "display(df_city_demographics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# I94 Immigration dataset analysis, for the month of April\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_i94 = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3096313 entries, 0 to 3096312\n",
      "Data columns (total 28 columns):\n",
      "cicid       float64\n",
      "i94yr       float64\n",
      "i94mon      float64\n",
      "i94cit      float64\n",
      "i94res      float64\n",
      "i94port     object\n",
      "arrdate     float64\n",
      "i94mode     float64\n",
      "i94addr     object\n",
      "depdate     float64\n",
      "i94bir      float64\n",
      "i94visa     float64\n",
      "count       float64\n",
      "dtadfile    object\n",
      "visapost    object\n",
      "occup       object\n",
      "entdepa     object\n",
      "entdepd     object\n",
      "entdepu     object\n",
      "matflag     object\n",
      "biryear     float64\n",
      "dtaddto     object\n",
      "gender      object\n",
      "insnum      object\n",
      "airline     object\n",
      "admnum      float64\n",
      "fltno       object\n",
      "visatype    object\n",
      "dtypes: float64(13), object(15)\n",
      "memory usage: 661.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_i94.info())\n",
    "display(df_i94.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This analysis will only be focused on the columns that uniquely identify each row, for each dataset. The resto of the columns will not be analysed, because empty or duplicate values will tolerated for this project and may be handled at a higher layer, i.e., on queries designed for specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Airport Codes Table dataset analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Aiport Codes present in column 'ident': 0\n",
      "Duplicate Aiport Codes present in column 'ident': 0\n"
     ]
    }
   ],
   "source": [
    "# 'ident' uniquely identifies each airport. We will check for duplicate and null values\n",
    "print(\"Unique Aiport Codes present in column 'ident': {}\".format(df_airport.ident.isna().sum()))\n",
    "print(\"Duplicate Aiport Codes present in column 'ident': {}\".format(len(df_airport.ident)-len(df_airport.ident.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__US City Demographics dataset analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Cities present in column 'City': 0\n",
      "Duplicate Cities present in column 'City': 2324\n"
     ]
    }
   ],
   "source": [
    "# 'City' uniquely identifies each airport. We will check for duplicate and null values\n",
    "print(\"Unique Cities present in column 'City': {}\".format(df_city_demographics.City.isna().sum()))\n",
    "print(\"Duplicate Cities present in column 'City': {}\".format(len(df_city_demographics.City)-len(df_city_demographics.City.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This dataset has a high number of duplicate entries because the same Cities are represented multiples times to display information of ethnical population. \n",
    "\n",
    "In Amazon Redshift, when transfering staging data from S3 to final tables, ethnical population count is merged into the same line and new columns are created. The new created columns are the count of the following population ethnical groups: white, black, asian, native and latino.\n",
    "\n",
    "In the following chapters this procedure can be consulted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__i94 Immigration dataset analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Aiport Codes present in column 'ident': 0\n",
      "Duplicate Aiport Codes present in column 'ident': 0\n"
     ]
    }
   ],
   "source": [
    "# 'cicid' uniquely identifies each i94 immigration record. We will check for duplicate and null values\n",
    "print(\"Unique Record IDs present in column 'cicid': {}\".format(df_i94.cicid.isna().sum()))\n",
    "print(\"Duplicate Record IDs present in column 'cicid': {}\".format(len(df_i94.cicid)-len(df_i94.cicid.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "First, raw data was loaded to the following staging tables in Redshift, from the CSV files:\n",
    "* Staging_airport_codes: Airport code table\n",
    "* Staging_demographics: US City Demographic dataset\n",
    "* Staging_country_codes: Country code dataset obtained from the provided SAS Labels Description\n",
    "* Staging_i94_data: I94 immigration dataset\n",
    "\n",
    "The datasets and converted to the appropriate format in the final dimension and fact tables, when loaded from staging tables.\n",
    "**************************\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The following SQL queries were used to load data from staging to the dimension and fact tables\n",
    "\n",
    "# Countries data was not didn't need any special arrangement\n",
    "countries_dim_table_insert = (\"\"\"\n",
    "    SELECT code, country\n",
    "    FROM staging_country_codes;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# Cities demograpghics table was redesigned. In the raw data, Cities had multiple duplicate entries with \n",
    "# exception of population race data. Therefore, new columns were created in the Cities Demographics dimensional table\n",
    "# to store this data without the need of duplicate entries of the same city.\n",
    "cities_demographics_dim_table_insert = (\"\"\"\n",
    "    SELECT      city,\n",
    "                state,\n",
    "                median_age,\n",
    "                male_pop,\n",
    "                female_pop,\n",
    "                total_pop,\n",
    "                veterans,\n",
    "                foreign_born,\n",
    "                avg_house_size,\n",
    "                MAX(CASE WHEN race = 'White' THEN count ELSE 0 END) white_pop,\n",
    "                MAX(CASE WHEN race = 'Asian' THEN count ELSE 0 END) asian_pop,\n",
    "                MAX(CASE WHEN race = 'Black or African-American' THEN count ELSE 0 END) black_pop,\n",
    "                MAX(CASE WHEN race = 'American Indian and Alaska Native' THEN count ELSE 0 END) native_pop,\n",
    "                MAX(CASE WHEN race = 'Hispanic or Latino' THEN count ELSE 0 END) latino_pop        \n",
    "    FROM        staging_demographics\n",
    "    GROUP BY    city,\n",
    "                state,\n",
    "                median_age,\n",
    "                male_pop,\n",
    "                female_pop,\n",
    "                total_pop,\n",
    "                veterans,\n",
    "                foreign_born,\n",
    "                avg_house_size\n",
    "    ;\n",
    "    \"\"\")\n",
    "\n",
    "# Immigration data was loaded to the arrivals fact table. Date fields were converted from SAS date format to \n",
    "# YYYY-MM-DD format. Also, the 'dtaddto' was a string that had to be verified and converted in Redshift when loaded\n",
    "# to the fact table\n",
    "arrivals_fact_table_insert = (\"\"\"\n",
    "    SELECT      cicid,\n",
    "                i94yr,\n",
    "                i94mon,\n",
    "                admnum,\n",
    "                date ('1960-01-01'::date + arrdate * interval '1 day') as arrdate,\n",
    "                i94res,\n",
    "                i94port,\n",
    "                i94visa,\n",
    "                i94mode,\n",
    "                i94bir,\n",
    "                gender,\n",
    "                CASE\n",
    "                    WHEN (dtaddto BETWEEN 00000000 AND 12312999) THEN to_date(dtaddto::varchar,'MMDDYYYY')\n",
    "                    ELSE NULL\n",
    "                END,\n",
    "                date ('1960-01-01'::date + depdate * interval '1 day')\n",
    "    FROM        staging_i94_data\n",
    "    WHERE       i94res IS NOT NULL;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The used data model is the following:\n",
    "<img src=\"images/data_model.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "It was choosen a Star Schema for this data warehouse, in order to simplify the queries and for easier maintenance.\n",
    "* Fact Table\n",
    " * arrivals: holds the factual and quantifiable data associated to immigrant entry in the US.\n",
    " \n",
    " \n",
    "* Dimension Tables:\n",
    " * us_airports: holds the data for each airport where immigrants entry to the US.\n",
    " * cities_demographics: it stores demographics data for US cities.\n",
    " * countries: stores the country codes used in i94 forms, and the corresponding country name.\n",
    " * arrival_modes: it stores the arrival modes of the transportation used by the immigrants (e.g.: air, sea).\n",
    " * visa_types: stores the visa types assigned to immigrants (e.g.: business, pleasure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The designed data pipeline is the following:\n",
    "<img src=\"images/data_pipeline.png\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The main data pipeline has the following stages:\n",
    "* staging_tables_create: The staging tables are created to store raw data from source CSV files in S3.\n",
    "* stage_target table: The staging tables are loaded with the copy command from source S3 csv files.\n",
    "* dim_\"tables\"_create: The dimension tables are created in Redshift.\n",
    "* Load_\"target_table\"_dim_table: The dimension tables are populated with data from staging tables. Some data cleaning can happen in this stage (e.g.: null or invalid values are handled).\n",
    "* fact_tables_create: The fact table is created in Redshift.\n",
    "* Load_\"target_table\"_fact_table: The fact table is populated with data from staging tables. Some data cleaning and conversion (e.g.: dates) are handled at this stage\n",
    "* Run_quality_checks: Some quality checks are done to the fact and dimension tables, to ensure the data pipeline was executed successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "A manual procedure to upload the source files to S3 is done before running the data pipeline. The following is done:\n",
    "* i94 immigration SAS data is converted to CSV usign pandas, because Redshift doesn't support SAS file format.\n",
    "* \"i94_country_codes.csv\" file is created and copied manually from the provided data dictionary file \"I94_SAS_Labels_Descriptions.SAS\". This is done because the file was not possible to be loaded to pandas due to SAS format errors. This file contains valid Country Codes used in i94 immigration dataset fields.\n",
    "* All CSV files were uploaded manually to S3 from this project's workspace.\n",
    "\n",
    "Below is the code used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# S3 boto3 client configuration with Access and Secret Keys\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "bucket_name = 'dchainho-udacity-dend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CSV files upload to S3 using boto3\n",
    "s3.upload_file(\n",
    "    \"us-cities-demographics.csv\", \n",
    "    bucket_name, \n",
    "    \"capstone/dataset/us-cities-demographics.csv\")\n",
    "s3.upload_file(\n",
    "    \"airport-codes_csv.csv\", \n",
    "    bucket_name, \n",
    "    \"capstone/dataset/airport-codes_csv.csv\")\n",
    "s3.upload_file(\n",
    "    \"i94_country_codes.csv\", \n",
    "    bucket_name, \n",
    "    \"capstone/dataset/i94_country_codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# i94 pandas file saved to CSV file and upload to S3\n",
    "df_i94.to_csv(\"i94_csv/apr16.csv\", index=False)\n",
    "s3.upload_file(\"i94_csv/apr16.csv\", bucket_name, \"capstone/dataset/i94/csv/apr16.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data pipeline was built in Airflow as follows:\n",
    "\n",
    "* dend-capstone.py: Main airflow config file where the DAGs and data pipeline are created.\n",
    "* stage_redshift.py: Custom operator to create staging DAGs.\n",
    "* load_dimension.py: Custom operator to create DAGs to load dimension tables.\n",
    "* load_fact.py: Custom operator to create DAG to load fact table.\n",
    "* data_quality.py: Custom operator for data quality checks DAGs.\n",
    "* sql_queries.py: Helper file containing all the SQL queries used in the Redshift.\n",
    "\n",
    "The files are located in 'airflow/' folder and sub-folders, where they can consulted in depth.\n",
    "\n",
    "The data pipeline definition can be seen on the code snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_operator = DummyOperator(task_id='Begin_execution',  dag=dag)\n",
    "\n",
    "# Staging tables creation in Redshift\n",
    "staging_tables_create = PostgresOperator(\n",
    "    dag=dag,\n",
    "    task_id='staging_tables_create',\n",
    "    postgres_conn_id='redshift',\n",
    "    sql=SqlQueries.staging_tables_create,\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "# Staging raw data from csv files in S3 to Redshift staging tables\n",
    "stage_airports_to_redshift = StageToRedshiftOperator(\n",
    "    task_id='Stage_airports',    \n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"staging_airport_codes\",\n",
    "    s3_bucket=\"dchainho-udacity-dend\",\n",
    "    s3_file=\"capstone/dataset/airport-codes_csv.csv\",\n",
    "    iam_role=IAM_ROLE\n",
    ")\n",
    "\n",
    "stage_demographics_to_redshift = StageToRedshiftOperator(\n",
    "    task_id='Stage_demographics',    \n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"staging_demographics\",\n",
    "    s3_bucket=\"dchainho-udacity-dend\",\n",
    "    s3_file=\"capstone/dataset/us-cities-demographics.csv\",\n",
    "    iam_role=IAM_ROLE,\n",
    "    delimiter=\";\"\n",
    ")\n",
    "\n",
    "stage_i94_data_to_redshift = StageToRedshiftOperator(\n",
    "    task_id='Stage_i94_data',    \n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"staging_i94_data\",\n",
    "    s3_bucket=\"dchainho-udacity-dend\",\n",
    "    s3_file=\"capstone/dataset/i94/csv/\",\n",
    "    iam_role=IAM_ROLE\n",
    ")\n",
    "\n",
    "stage_countries_to_redshift = StageToRedshiftOperator(\n",
    "    task_id='Stage_countries',    \n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"staging_country_codes\",\n",
    "    s3_bucket=\"dchainho-udacity-dend\",\n",
    "    s3_file=\"capstone/dataset/i94_country_codes.csv\",\n",
    "    iam_role=IAM_ROLE\n",
    ")\n",
    "\n",
    "# Dimension tables creation in Redshift\n",
    "dim_tables_create = PostgresOperator(\n",
    "    dag=dag,\n",
    "    task_id='dim_tables_create',\n",
    "    postgres_conn_id='redshift',\n",
    "    sql=SqlQueries.dim_tables_create,\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "# Inserting data in dimensional tables, from the staging tables in Redshift\n",
    "load_visa_types_dim_table = PostgresOperator(\n",
    "    dag=dag,\n",
    "    task_id='Load_visa_types_dim_table',\n",
    "    ui_color = '#358140',\n",
    "    postgres_conn_id='redshift',\n",
    "    sql=SqlQueries.load_visa_types,\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "load_arrival_modes_dim_table = PostgresOperator(\n",
    "    dag=dag,\n",
    "    task_id='Load_arrival_modes_dim_table',\n",
    "    ui_color = '#358140',\n",
    "    postgres_conn_id='redshift',\n",
    "    sql=SqlQueries.load_arrival_modes,\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "load_countries_dim_table = LoadDimensionOperator(\n",
    "    task_id='Load_countries_dim_table',\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"countries\",    \n",
    "    insert_sql_select=SqlQueries.countries_dim_table_insert\n",
    ")\n",
    "\n",
    "load_cities_demographics_dim_table = LoadDimensionOperator(\n",
    "    task_id='Load_cities_demographics_dim_table',\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"cities_demographics\",    \n",
    "    insert_sql_select=SqlQueries.cities_demographics_dim_table_insert\n",
    ")\n",
    "\n",
    "load_us_airports_dim_table = LoadDimensionOperator(\n",
    "    task_id='Load_us_airports_dim_table',\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"us_airports\",    \n",
    "    insert_sql_select=SqlQueries.us_airports_dim_table_insert\n",
    ")\n",
    "\n",
    "# Facts table creation\n",
    "fact_tables_create = PostgresOperator(\n",
    "    dag=dag,\n",
    "    task_id='fact_tables_create',\n",
    "    postgres_conn_id='redshift',\n",
    "    sql=SqlQueries.fact_tables_create,\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "# Loading facts table with data from staging table\n",
    "load_arrivals_fact_table = LoadFactOperator(\n",
    "    task_id='Load_arrivals_fact_table',\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"arrivals\",    \n",
    "    insert_sql_select=SqlQueries.arrivals_fact_table_insert\n",
    ")\n",
    "\n",
    "# Quality checks on final dimensional and facts table\n",
    "run_quality_checks_dim_tables = DataQualityOperator(\n",
    "    task_id='Run_quality_checks_dim_tables',\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    tables=DIM_TABLES,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "run_quality_checks_fact_tables = DataQualityOperator(\n",
    "    task_id='Run_quality_checks_fact_tables',\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    tables=FACT_TABLES,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "end_operator = DummyOperator(task_id='Stop_execution',  dag=dag)\n",
    "\n",
    "\n",
    "stage_tables_list = [stage_countries_to_redshift,\n",
    "                     stage_airports_to_redshift, \n",
    "                     stage_demographics_to_redshift, \n",
    "                     stage_i94_data_to_redshift]\n",
    "\n",
    "load_dim_tables_list = [load_countries_dim_table, \n",
    "                        load_visa_types_dim_table, \n",
    "                        load_arrival_modes_dim_table,\n",
    "                        load_cities_demographics_dim_table]\n",
    "\n",
    "# Data pipeline creation\n",
    "start_operator >> staging_tables_create \n",
    "staging_tables_create >> stage_tables_list >> dim_tables_create\n",
    "dim_tables_create >> load_dim_tables_list\n",
    "load_cities_demographics_dim_table >> load_us_airports_dim_table \n",
    "load_dim_tables_list >> fact_tables_create\n",
    "load_us_airports_dim_table >> fact_tables_create\n",
    "fact_tables_create >> load_arrivals_fact_table\n",
    "load_arrivals_fact_table >> [run_quality_checks_dim_tables, run_quality_checks_fact_tables] >> end_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "At the end, 6 quality checks are done, by counting the rows of all 6 created tables. If any table has zero count, an error is raised.\n",
    "The code of the customer operator to create the DAGs can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DAGs created in the data pipeline\n",
    "run_quality_checks_dim_tables = DataQualityOperator(\n",
    "    task_id='Run_quality_checks_dim_tables',\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    tables=DIM_TABLES,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "run_quality_checks_fact_tables = DataQualityOperator(\n",
    "    task_id='Run_quality_checks_fact_tables',\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    tables=FACT_TABLES,\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Custom operator DAG count procedure\n",
    "def execute(self, context):\n",
    "        redshift = PostgresHook(postgres_conn_id=self.redshift_conn_id)  \n",
    "\n",
    "        for t in self.tables:\n",
    "            logging.info(f\"Data quality check -> table '{t}' -> verifying row count.\")\n",
    "            sql_query=DataQualityOperator.count_rows.format(t)        \n",
    "            records = redshift.get_records(sql_query)\n",
    "\n",
    "            if len(records) < 1 or len(records[0]) < 1:\n",
    "                raise ValueError(f\"Data quality check -> table '{t}' -> FAILED, returned no results.\")\n",
    "            num_records = records[0][0]\n",
    "            if num_records < 1:\n",
    "                raise ValueError(f\"Data quality check -> table '{t}' -> FAILED, contained 0 rows.\")\n",
    "            logging.info(f\"Data quality check -> table '{t}' -> PASSED, with {records[0][0]} records.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dimensional Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/us_airports.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/cities_demographics.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/countries.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/visa_types.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/arrival_modes.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Fact Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/arrivals.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "The tools used in this project were:\n",
    "- Airflow: For defining and executing the data pipeline. Airflow was chosen because it provides a highly customizable and programmatic way to build workflows for data processing. It also provides a solid way to automate queries for future needs.\n",
    "- Amazon S3: To store the raw data files to be used in Amazon Redshift. A cloud solution, very cost efficient, and it provides fast loading of raw data to staging tables in Redshift.\n",
    "- Amazon Redshift: A cloud service provided by Amazon to host SQL databases. This technology was used because it provides a way for fast scaling, being a cost effective solution and it has fast queries that together with SQL, provides a powerful for joining tables for customizable queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Propose how often the data should be updated and why.\n",
    "\n",
    "The Data Warehouse in Reshift could be updated at the end of each day, with the data from all the immigrant arrivals in that period. This can be easily done with Airflow by scheduling DAGs and run the data pipeline automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x\n",
    "  * Amazon Redshift is a powerful cloud solution highly scalable. For this project, only 1 cluster was used. If the data was 100x bigger, I would use a higer number of clusters in order to support and parallelize the increased number of queries in order to maintain reduced latencies.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "  * A solution used in this project is already provided for this scenario. Airflow is an excellent tool to run scheduled automatic workflows. In this case, a DAG should be created to update the tables with the previous day new arrivals, every day at, for example, 2am, for all the tables to be finished in time to be used by the dashboard at 7am every day.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "  * Amazon Redshift probably has the capacity to handle the increase query load. Imagining a scenario of 100 users and 20 daily customized queries per use, this would ammount to a total of 2000 queries per day. Dimensioning for a peak simultaneous querying of 75%, i.e., 1500 simultaneous and different queries, generating 1500 temporary tables, Amazon Redshift would be more than enough. From Amazon Redshift site, \"The maximum number of tables is 9,900 for large and xlarge cluster node types and 20,000 for 8xlarge cluster node types.\". The number of queries could increase 10x or 20x times and still Redshift would be capable of supporting such load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
